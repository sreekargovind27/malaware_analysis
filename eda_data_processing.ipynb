{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd73b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas matplotlib seaborn tqdm fastparquet python-snappy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81b32f",
   "metadata": {},
   "source": [
    "## Importing Libraries and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b340f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary libraries are installed and imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import fastparquet\n",
    "import gc\n",
    "\n",
    "# Configure plotting style and suppress warnings\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All necessary libraries are installed and imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356d7ea3",
   "metadata": {},
   "source": [
    "## Configuration and Data Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b77475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 18 scenarios ready for processing.\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_DIR = Path(\"iot23_csv_data\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "PROCESSED_DATA_DIR = Path(\"processed_data\")\n",
    "\n",
    "# Create output directories\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Discover scenario files\n",
    "scenarios = {}\n",
    "if not BASE_DATA_DIR.exists():\n",
    "    print(f\"FATAL ERROR: The directory '{BASE_DATA_DIR}' does not exist.\")\n",
    "else:\n",
    "    for path in BASE_DATA_DIR.iterdir():\n",
    "        if path.is_dir() and path.name.startswith(\"CTU-\"):\n",
    "            log_file = path / 'conn.log.labeled.csv'\n",
    "            if log_file.exists():\n",
    "                scenarios[path.name] = log_file\n",
    "\n",
    "print(f\"Discovered {len(scenarios)} scenarios ready for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62c0b69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CTU-IoT-Malware-Capture-20-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-20-1/conn.log.labeled.csv'),\n",
       " 'CTU-Honeypot-Capture-4-1': PosixPath('iot23_csv_data/CTU-Honeypot-Capture-4-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-52-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-52-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-60-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-60-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-8-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-8-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-49-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-49-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-3-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-3-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-21-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-21-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-9-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-9-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-48-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-48-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-42-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-42-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-1-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-1-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-36-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-36-1/conn.log.labeled.csv'),\n",
       " 'CTU-Honeypot-Capture-5-1': PosixPath('iot23_csv_data/CTU-Honeypot-Capture-5-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-44-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-44-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-34-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-34-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-35-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-35-1/conn.log.labeled.csv'),\n",
       " 'CTU-IoT-Malware-Capture-7-1': PosixPath('iot23_csv_data/CTU-IoT-Malware-Capture-7-1/conn.log.labeled.csv')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a044b8",
   "metadata": {},
   "source": [
    "## Master Parquet File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e8621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting memory-safe combination to Parquet: processed_data/master_iot23_data.parquet ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining Scenarios:  83%|████████▎ | 15/18 [04:25<00:43, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - An error occurred while processing CTU-IoT-Malware-Capture-34-1: Error converting column \"ts\" to bytes using encoding None. Original error: could not convert string to float: 'CrDn63WjJEmrWGjqf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining Scenarios: 100%|██████████| 18/18 [06:16<00:00, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created master Parquet file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "combined_parquet_path = PROCESSED_DATA_DIR / 'master_iot23_data.parquet'\n",
    "chunksize = 250000  \n",
    "is_first_chunk = True\n",
    "\n",
    "print(f\"--- Starting memory-safe combination to Parquet: {combined_parquet_path} ---\")\n",
    "\n",
    "for scenario_name, file_path in tqdm(scenarios.items(), desc=\"Combining Scenarios\"):\n",
    "    try:\n",
    "        # Create an iterator that reads the CSV in chunks\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            file_path, \n",
    "            chunksize=chunksize, \n",
    "            low_memory=False, \n",
    "            na_values='-' \n",
    "        )\n",
    "        \n",
    "        for chunk in chunk_iterator:\n",
    "            chunk['scenario'] = scenario_name\n",
    "            \n",
    "            if is_first_chunk:\n",
    "                # For the very first chunk, create a new Parquet file\n",
    "                fastparquet.write(\n",
    "                    combined_parquet_path, \n",
    "                    chunk, \n",
    "                    compression='SNAPPY',\n",
    "                    append=False \n",
    "                )\n",
    "                is_first_chunk = False\n",
    "            else:\n",
    "                # For all subsequent chunks, append to the existing file\n",
    "                fastparquet.write(\n",
    "                    combined_parquet_path, \n",
    "                    chunk, \n",
    "                    compression='SNAPPY',\n",
    "                    append=True\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"  - An error occurred while processing {scenario_name}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created master Parquet file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2c73b",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-calculating global minimum timestamp...\n",
      "Global minimum timestamp found: 2018-05-09 15:30:31.015073061\n",
      "\n",
      "--- Starting memory-safe feature engineering to: processed_data/enriched_iot23_data.parquet ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching Data: 313it [05:01,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created final enriched Parquet file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "master_parquet_path = PROCESSED_DATA_DIR / 'master_iot23_data.parquet'\n",
    "enriched_parquet_path = PROCESSED_DATA_DIR / 'enriched_iot23_data.parquet'\n",
    "\n",
    "# Pre-calculate the global minimum timestamp for the 'time_since_start' feature\n",
    "print(\"Pre-calculating global minimum timestamp...\")\n",
    "min_ts = fastparquet.ParquetFile(master_parquet_path).to_pandas(['ts'])['ts'].min()\n",
    "global_min_datetime = pd.to_datetime(min_ts, unit='s')\n",
    "print(f\"Global minimum timestamp found: {global_min_datetime}\")\n",
    "\n",
    "# Main Processing Loop \n",
    "pf = fastparquet.ParquetFile(master_parquet_path)\n",
    "is_first_chunk = True\n",
    "\n",
    "print(f\"\\n--- Starting memory-safe feature engineering to: {enriched_parquet_path} ---\")\n",
    "\n",
    "for chunk_df in tqdm(pf.iter_row_groups(), desc=\"Enriching Data\"):\n",
    "    try:\n",
    "        # Feature Engineering: Temporal Features \n",
    "        chunk_df.dropna(subset=['ts'], inplace=True)\n",
    "        chunk_df['datetime'] = pd.to_datetime(chunk_df['ts'], unit='s', errors='coerce')\n",
    "        chunk_df['hour'] = chunk_df['datetime'].dt.hour\n",
    "        chunk_df['day_of_week'] = chunk_df['datetime'].dt.dayofweek\n",
    "        chunk_df['time_since_start'] = (chunk_df['datetime'] - global_min_datetime).dt.total_seconds()\n",
    "\n",
    "        # Feature Engineering: Standardize Labels & Phases \n",
    "        if 'label' in chunk_df.columns:\n",
    "            chunk_df['label'].fillna('Unknown', inplace=True)\n",
    "            label_mapping = {\n",
    "                'Benign': 'Benign', 'benign': 'Benign', 'Malicious': 'Malicious', 'C&C': 'Command_Control',\n",
    "                'CC': 'Command_Control', 'DDoS': 'DDoS_Attack', 'DDOS': 'DDoS_Attack',\n",
    "                'PartOfAHorizontalPortScan': 'Port_Scan', 'PartOfHorizontalPortScan': 'Port_Scan',\n",
    "                'Attack': 'Direct_Attack', 'FileDownload': 'File_Download', 'HeartBeat': 'Heartbeat',\n",
    "                'Okiru': 'Okiru_Activity', 'Mirai': 'Mirai_Activity', 'Torii': 'Torii_Activity'\n",
    "            }\n",
    "            chunk_df['standardized_label'] = chunk_df['label'].astype(str).map(label_mapping).fillna(chunk_df['label'])\n",
    "            \n",
    "            def classify_attack_phase(label):\n",
    "                label = str(label).lower()\n",
    "                if 'benign' in label: return 'Normal_Activity'\n",
    "                elif 'scan' in label: return 'Reconnaissance'\n",
    "                elif 'command' in label or 'control' in label: return 'C&C_Communication'\n",
    "                elif 'ddos' in label: return 'Denial_of_Service'\n",
    "                elif 'attack' in label: return 'Direct_Attack'\n",
    "                elif 'download' in label: return 'Payload_Delivery'\n",
    "                else: return 'Other_Malicious'\n",
    "            chunk_df['attack_phase'] = chunk_df['standardized_label'].apply(classify_attack_phase)\n",
    "\n",
    "        # Feature Engineering: Calculate Flow Metrics \n",
    "        numeric_cols = ['duration', 'orig_bytes', 'resp_bytes', 'orig_pkts', 'resp_pkts']\n",
    "        for col in numeric_cols:\n",
    "            if col in chunk_df.columns: chunk_df[col].fillna(0, inplace=True)\n",
    "        \n",
    "        chunk_df['total_bytes'] = chunk_df['orig_bytes'] + chunk_df['resp_bytes']\n",
    "        if 'conn_state' in chunk_df.columns:\n",
    "            chunk_df['connection_successful'] = chunk_df['conn_state'].isin(['SF', 'S1', 'S2', 'S3'])\n",
    "\n",
    "        # Write the enriched chunk to the new Parquet file \n",
    "        if is_first_chunk:\n",
    "            fastparquet.write(enriched_parquet_path, chunk_df, compression='SNAPPY', append=False)\n",
    "            is_first_chunk = False\n",
    "        else:\n",
    "            fastparquet.write(enriched_parquet_path, chunk_df, compression='SNAPPY', append=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  - An error occurred while processing a chunk: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully created final enriched Parquet file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cabf2b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del chunk_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649cd0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
